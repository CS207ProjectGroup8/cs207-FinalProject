{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "WSQM0RC667My",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ]
    },
    {
      "metadata": {
        "id": "VkMk5ckc76hR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The software deploys the Forward Mode of Automatic Differentiation (Forward AD) to evaluate the derivatives of functions. Specifically, the software efficiently evaluates at machine precision the Jacobian matrix (a $m\\times n$ matrix) of any function $f: R^n \\rightarrow R^m$.\n",
        "\n",
        "**Importance of the Jacobian Matrix**\n",
        "\n",
        "Evaluating the Jacobian matrix is significant. The Jacobian matrix provides information of (partial) derivatives of the function, which can be used in many different ways. For example, it can be used to linearly approximate the function about the differentiation point, and it can be used to find the function’s extrema and roots. Moreover, Jacobian matrix is critical in a lot of optimization problems across different scientific fields.  \n",
        "\n",
        "One example of the utilization of Jacobian matrix is the famous Newton’s Method for root finding. The core of the method is that it utilizes evaluation of the Jacobian matrix in each step of its iterations to linearly approximate the given function.\n",
        "\n",
        "**Importance of High Accuracy and Efficiency**\n",
        "\n",
        "It is important to evaluate the Jacobian matrix with high accuracy, as small errors could accumulate in higher dimensions and over iterations. It is also important to evaluate the Jacobian matrix efficiently, considering the time cost of the calculation as the complexity and dimension of the function go up.\n",
        "\n",
        "**Problems with Classical Methods**\n",
        "\n",
        "Both Symbolic Differentiation and Finite Difference Method face issues of increasing errors and increasing time cost in evaluating the derivatives as dimensions and complexities of the function go up.\n",
        "\n",
        "**Advantages of the Software (Forward AD)**\n",
        "\n",
        "Our software, by using Forward AD, can evaluate the Jacobian Matrix of any function $f: R^n \\rightarrow R^m$ up to machine precision efficiently. Every function, regardless of its complexity, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication and division) and elementary functions (sin, cos, log, etc). Using this fact, AD applies chain rule repeatedly on these operations, and can therefore compute derivatives of any order up to machine precision of the given function. As the order increases, the complexity of AD calculation is not worse than the original function, therefore achieving efficiency. \n"
      ]
    },
    {
      "metadata": {
        "id": "A4uQrwlG7BwB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Background"
      ]
    },
    {
      "metadata": {
        "id": "ONUjKDh8-BtX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Chain Rule**\n",
        "The chain rule is used to differentiate composite functions. The simplest case is that: $y = f(g(x)) = f(g(x_0)) = f(w_1) = w_2$:\n",
        "\n",
        "$\\frac{df(g(x))}{dx} = f'(g(x))g'(x) = \\frac{dy}{dw_1} \\frac{dw_1}{dx}$ \n",
        "\n",
        "Forward AD implements the chain rule from inside to outside: \n",
        "First computing $\\frac{dw_1}{dx}$, then $\\frac{dy}{dw_1}$.\n",
        "\n",
        "This can be generalized to a more complicated function with increased dimensions and multiple variables, that is, applying chain rule repeatedly on each operation with regard to different variables, for any $f: R^n \\rightarrow R^m$, we can computer the Jacobian matrix of $f$, where $J \\in M_{m \\times n}$. \n",
        "\n",
        "**The Graph Structure of Calculations**\n",
        "\n",
        "We can visualize the function inputs and its evaluation process with a computational graph. \n",
        "A simple example is: Consider $f: R^2 \\rightarrow R^1$, $f(x, y) = x + 3y-sin(x)$. \n",
        "\n",
        "*Computational graph: *\n",
        "![alt text](https://)\n",
        "\n",
        "When evaluating $\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}$, we can also draw a table to visualize the evaluation trave when applying the chain rule. For example, calculate $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ at $(x, y) = (2, 1)$: \n",
        "\n",
        "|      | Elementary Operations   | Numerical Values    |  $\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}$  |   Numerical Value  $\\frac{\\partial f}{\\partial x}$   |   Numerical Value  $\\frac{\\partial f}{\\partial y}$     |\n",
        "| ------------- |:-------------:| -----:| ------------- |:-------------:| -----:|\n",
        "|    $x_1$  | $x=2$  | 2 |  $\\dot{x_1}$ | 1  | 0  |\n",
        "|    $x_2$  | $y=1$  | 1 | $\\dot{x_2}$  | 0  |  1  |\n",
        "|    $x_3$  | $3x_2$  | 3 | $3\\dot{x_2}$  | 0  |  3  |\n",
        "|    $x_4$  |  $x_1 + x_3$ | 5 | $\\dot{x_1} + \\dot{x_3}$  | 1  | 3  |\n",
        "|    $x_5$  | $sin(x_1)$  | $sin2$ | $cos(x_1)\\dot{x_1}$  | $cos(2)$  | 0  |\n",
        "|    $x_6$  | $-x_5$  | $-sin2$ | $-\\dot{x_5}$  | $-cos(2)$  | 0  |\n",
        "|    $x_7$  | $x_4 + x_6$  | $5-sin2$ | $\\dot{x_4} + \\dot{x_6}$  | $1-cos(2)$  |  3 |\n",
        "\n",
        "\n",
        "$\\therefore \\frac{\\partial f(2, 1)}{\\partial x} = 1- cos(2)$; $\\frac{\\partial f(2, 1)}{\\partial y} = 3$. \n",
        " \n",
        " \n",
        "\n",
        "**Differentiation Rules of Elementary Operations**\n",
        "![alt text](https://)\n",
        "\n",
        "\n",
        "\n",
        "**Derivatives of Elementary Functions**\n",
        "\n",
        "**Dual Numbers**\n",
        "\n",
        "*Definition:* $z = x + \\epsilon x'$, where $\\epsilon ^2 = 0$ and $x, x' \\in \\mathbb{R}$. $x$ is the real part, and $x'$ is the dual part.\n",
        "\n",
        "We can use dual numbers to calcuate derivative of a function. \n",
        "\n",
        "For example, for $y = f(x)$, evaluating $f(x+ \\epsilon x') = u + v \\epsilon$, and we will have $f(x) = u$ and $\\frac{df}{dx} = v$. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "THhyoL3r7EZW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PackageName\n",
        "\n",
        "All modules can be accessed with:\n",
        "\n",
        "`from AD_package import *`\n",
        "\n",
        "The module for forward mode automatic differentiation can be accessed with:\n",
        "\n",
        "```\n",
        "from AD_package.ForwardAutoDiff import ElementaryFunctions as adf\n",
        "from AD_package.ForwardAutoDiff import ForwardAutoDiff as ad\n",
        "```\n",
        "\n",
        "AD objects can be instantiated and used as shown in the example below:\n",
        "\n",
        "```\n",
        "# User inputs the number of variables in the function and the value at which they should be evaluated\n",
        "x = ad(1)\n",
        "y = ad(2)\n",
        "\n",
        "# User inputs the function that needs to be evaluated using AD defined functions\n",
        " f = x + adf.cos(y)\n",
        "\n",
        "# To retrieve the value and derivative, user calls .val and .der\n",
        "print(f.val, f.der)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "DfIFEdfb7GvT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Software Organization\n",
        "\n",
        "Here is our proposed directory structure.\n",
        "  ```\n",
        "     AD_package\\\n",
        "           AD_package\\\n",
        "                 __init__.py\n",
        "                 ForwardAutoDiff\\\n",
        "                      ElementaryFunctions.py\n",
        "                      AutoDiffObject.py\n",
        "                 *\n",
        "                 tests\\\n",
        "                      __init__.py\n",
        "                      test_AutoDiffElementaryFunctions.py\n",
        "                      test_AutoDiffObject.py\n",
        "                      *\n",
        "                 docs\\\n",
        "                      milestone1.ipynb\n",
        "                      *\n",
        "           README.md\n",
        "           setup.py\n",
        "           LICENSE\n",
        "  ```\n",
        " Our package features two main submodules under the `ForwardAutoDiff` module:\n",
        "  * `ElementaryFunctions`\n",
        "    * This module will redefine all of the elementary functions to return both their values and their derivates when they are used. Values will be returned with `numpy` and derivatives will be returned with a combination of `numpy` and simple symbolic differentation.\n",
        "  * `ForwardAutoDiff`\n",
        "    * This module will implement the forward mode of automatic differentiation. Users will instantiate the variables of their desired function as `ForwardAutoDiff` objects and will insert these objects into equations in order to compute the value and derivative of that function.\n",
        "    \n",
        "For our test suite we will use both TravisCI and Coveralls: TravisCI to run tests and Coveralls to gauge code coverage. Our tests, which will live inside of a separate folder in our directory, will be broken up by module. For distribution, we will use the Python Package Index (PyPI).\n",
        "\n",
        "For code documentation, we will include substantive docstrings that include: \n",
        "* listing the basic function of each class or method,\n",
        "* defining what the expected input and output are,\n",
        "* and providing a basic example of the class or method (multiple examples will be included if necessary).\n",
        "\n",
        "Note that other modules and tests may be added depending on what extensions we choose to implement, and other docs will be added along the way for course requirements and to capture any other documentation we wish to include. A placeholder `*` has been left in place of those additions for now."
      ]
    },
    {
      "metadata": {
        "id": "xbbNJgrZ7J1H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "###Core Data Structure:\n",
        "We will be using dictionary to store the derivatives of functions with respect to input vectors. For example, for each auto differentiator object we return, it would have ad.der = {x: , y: , z: ...} for the next node operation to access. \n",
        "\n",
        "###Classes to implement:\n",
        "\n",
        "\n",
        "class ElementaryFunctions():\n",
        "        def __init__(self)"
      ]
    },
    {
      "metadata": {
        "id": "4jNYMsvn7M0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Additional\n",
        "\n",
        "One potential extension of our project would be to improve the efficiency of the `ForwardAutoDiff` implementation by creating a smarter computational graph that saves necessary intermediate results to spare the algorithm from having to compute the same value and derivate for the same function again. \n",
        "\n",
        "For example, if we are interested in computing the derivative of the following function with respect to `x` and evaluated at `x=a`:\n",
        "\n",
        "`f = (x^2) + cos(x^2) + sin(x^2) + log(x^2) + (x^2)^(x^2)`\n",
        "\n",
        "it may be more efficient to compute the value and derivate of `x^2` evaluated at `a` and reuse this whenever `x^2` shows up, as opposed to computing the value and derivative of `x^2` from scratch every time, which is how our initial algorithm would work."
      ]
    }
  ]
}